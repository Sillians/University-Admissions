{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "!python3 -m pip install scipy==1.2 --upgrade\n",
    "# !pip3 list\n",
    "!pip3 uninstall statsmodels -y\n",
    "!pip3 install statsmodels==0.10.0rc2 --pre --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy==1.2.1 scikit-learn==0.22 tensorflow==2.0 keras==1.2.2 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install or update the pipelines SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your notebook kernel after updating the kfp sdk\n",
    "!pip3 install kfp --upgrade\n",
    "!pip3 install kfp --upgrade --user\n",
    "!pip install -U kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Restart the kernel before you proceed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after the pip install\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Check if the install was successful:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'University Admission notebook pipeline'        # Name of the experiment in the UI\n",
    "BASE_IMAGE = \"tensorflow/tensorflow:latest-gpu-py3\"    # Base image used for components in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp # the Pipelines SDK. \n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.components as comp\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "from kfp.dsl.types import Integer, GCSPath, String\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the outputs are stored\n",
    "out_dir = \"/home/jovyan/01-University-Admissions/data/out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pipeline Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='preprocess_op',\n",
    "    description='preprocessing function for University admission',\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "\n",
    "def preprocess(data_path):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import statsmodels.api as sm\n",
    "    import pylab\n",
    "    from scipy.stats import kstest, boxcox\n",
    "    from scipy.special import inv_boxcox\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    # Get data\n",
    "    DATA_PATH = \"https://raw.githubusercontent.com/HamoyeHQ/01-University-Admissions/master/data/\"\n",
    "\n",
    "    def load_admission_data(admission_path=DATA_PATH):\n",
    "        csv_path = os.path.join(admission_path, \"Admission_Predict_Ver1.1.csv\")\n",
    "        return pd.read_csv(csv_path)\n",
    "    \n",
    "    # load data from function\n",
    "    data = load_admission_data()\n",
    "    \n",
    "    \n",
    "    # Splitting into train and test\n",
    "    # Use the first 400 dataset for training and validation\n",
    "    train = data.iloc[:400, :]\n",
    "    # Use the last 100 dataset for testing and evaluation\n",
    "    test = data.iloc[400:, :]\n",
    "\n",
    "\n",
    "    # Splitting into features and Targets\n",
    "    X = train.drop(['Chance of Admit '], axis=1)\n",
    "    y = train['Chance of Admit ']\n",
    "\n",
    "    # Selecting only numeric features to test for normality\n",
    "    num_df = train.drop(['University Rating', 'Research', 'Chance of Admit '], axis=1)\n",
    "    \n",
    "    # check for normality using QQ-Plot\n",
    "    for cols in num_df.columns:\n",
    "        print(f'Q-Q Plot for {cols}')\n",
    "        sm.qqplot(num_df[cols], line='s')\n",
    "        pylab.show()\n",
    "    \n",
    "    # #Kolmogorov Smirnov Test for Normality\n",
    "    feature_list = list(num_df.columns)\n",
    "    for feature in feature_list:\n",
    "        output = kstest(num_df[feature], 'norm', N=100)\n",
    "        print(f'Kolmogorov Test for {feature} = {output}')\n",
    "\n",
    "    #output file to path\n",
    "    # NPZ is a file format by numpy that provides storage of array data using gzip compression. \n",
    "    # This imageio plugin supports data of any shape, and also supports multiple images per file.\n",
    "    np.savez_compressed(f'{data_path}/preprocessed-data.npz', \n",
    "                       train=train,\n",
    "                       test=test,\n",
    "                       X=X,\n",
    "                       y=y)\n",
    "    print(\"Preprocessing Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Function\n",
    "\n",
    "### Transform the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='transform_op',\n",
    "    description='transformation pipeline function for University admission',\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "\n",
    "def transform(data_path, pipeline_file):\n",
    "    \n",
    "    # Install all the dependencies inside the function\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    import statsmodels.api as sm\n",
    "    import pylab\n",
    "    from scipy.stats import kstest, boxcox\n",
    "    from scipy.special import inv_boxcox\n",
    "    from sklearn.preprocessing import PowerTransformer, OneHotEncoder, FunctionTransformer\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.compose import ColumnTransformer, TransformedTargetRegressor, make_column_transformer\n",
    "    from sklearn.base import TransformerMixin, BaseEstimator\n",
    "    from mlxtend.feature_selection import ColumnSelector\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    \n",
    "    # A Box Cox transformation is a transformation of a non-normal dependent variables into a normal shape. \n",
    "    # Normality is an important assumption for many statistical techniques; \n",
    "    # if your data isnâ€™t normal, applying a Box-Cox means that you are able to run a broader number of tests.\n",
    "    \n",
    "    #load the preprocessed data\n",
    "    preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "    train = preprocessed_data['train']\n",
    "    test = preprocessed_data['test']\n",
    "    X = preprocessed_data['X']\n",
    "    y = preprocessed_data['y']\n",
    "\n",
    "    # Function for transforming target variable\n",
    "    def func(target):\n",
    "        print('Target Transform Called')\n",
    "        target_ = target.copy()\n",
    "       # print(target.shape)\n",
    "        target_ = target_.flatten()\n",
    "       # print(target_.shape)\n",
    "        target_ = boxcox(target_, lmbda=1.6132074271235903)\n",
    "        tar = target_.reshape(-1,1)\n",
    "        return tar\n",
    "\n",
    "    # Function for reversing the transform performed on the target vatiable\n",
    "    # Read more on how to implement invboxcox (https://stackoverflow.com/questions/26391454/reverse-box-cox-transformation)\n",
    "    def invboxcox(target):\n",
    "        print('Inverse Target Transform Called')\n",
    "        ld = 1.6132074271235903\n",
    "        return(np.exp(np.log(ld*target+1)/ld))\n",
    "    \n",
    "    \n",
    "    # Custom transformer for changing data type of some features in the pipeline\n",
    "    class Typ_conv(BaseEstimator, TransformerMixin):\n",
    "\n",
    "        def fit(self, X, y=None, **fit_params):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, X):\n",
    "            cols = ['University Rating', 'Research']\n",
    "            for col in cols:\n",
    "                X[col] = X[col].astype('object')\n",
    "            V = pd.DataFrame(X)\n",
    "#             V.head(3)\n",
    "            return X.values\n",
    "        \n",
    "    # Creating transformer for numeric features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('power_transform', PowerTransformer(method='box-cox'))])\n",
    "\n",
    "    # Creating transformer for categorical features\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    # This selector allows the pipeliine select only specified features \n",
    "    selector = make_pipeline(ColumnSelector(cols=([1,2,3,4,5,6,7])))\n",
    "    \n",
    "    # Indices of Numeric Features\n",
    "    # Exclude the feature 'Serial No'\n",
    "    numeric_features = [0,1,3,4,5]\n",
    "    # Indices of Categorical Features\n",
    "    categorical_features = [2,6]\n",
    "    # Creating custom transformer for numerical and categorical features\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                               ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "   # Combining all transformers into a single pipeline\n",
    "    pipe = Pipeline(steps=[('tconv', Typ_conv()), \n",
    "                           ('select', selector), \n",
    "                           ('preprocessor', preprocessor), \n",
    "                           ('reg', BayesianRidge())])\n",
    "    \n",
    "    #Save the func function to the designated \n",
    "    with open(f'{data_path}/{pipeline_file}', 'wb') as func_file:\n",
    "        pickle.dump(func, func_file)\n",
    "    \n",
    "    \n",
    "    #Save the invboxcox function to the designated \n",
    "    with open(f'{data_path}/{pipeline_file}', 'wb') as invbox_file:\n",
    "        pickle.dump(invboxcox, invbox_file)\n",
    "    \n",
    "    \n",
    "     #Save the pipeline to the designated \n",
    "    with open(f'{data_path}/{pipeline_file}', 'wb') as file:\n",
    "        pickle.dump(pipe, file)\n",
    "        \n",
    "    print(\"Data transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function \n",
    "\n",
    "### Training the data with the BayesianRidge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='train_op',\n",
    "    description='training pipeline function for Graduate admission',\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "\n",
    "def train(data_path, model_file):\n",
    "    \n",
    "    # Install all the dependencies inside the function\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    import statsmodels.api as sm\n",
    "    import pylab\n",
    "    from scipy.stats import kstest, boxcox\n",
    "    from scipy.special import inv_boxcox\n",
    "    from sklearn.preprocessing import PowerTransformer, OneHotEncoder, FunctionTransformer\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.compose import ColumnTransformer, TransformedTargetRegressor, make_column_transformer\n",
    "    from sklearn.base import TransformerMixin, BaseEstimator\n",
    "    from mlxtend.feature_selection import ColumnSelector\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    \n",
    "    \n",
    "    #load the preprocessed data\n",
    "    preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "    train = preprocessed_data['train']\n",
    "    test = preprocessed_data['test']\n",
    "    X = preprocessed_data['X']\n",
    "    y = preprocessed_data['y']\n",
    "    \n",
    "    \n",
    "    # Load the saved pipe file\n",
    "    with open(f'{data_path}/{pipeline_file}', 'rb') as file:\n",
    "        pipe = pickle.load(file)\n",
    "        \n",
    "    # Load the saved func function\n",
    "    with open(f'{data_path}/{pipeline_file}', 'rb') as func_file:\n",
    "        func = pickle.load(func_file)\n",
    "        \n",
    "    # Load the saved invboxcox function\n",
    "    with open(f'{data_path}/{pipeline_file}', 'rb') as invbox_file:\n",
    "        invboxcox = pickle.load(invbox_file)\n",
    "\n",
    "    # We use Transformed Target Regressor because we are performing some transformation on the target variable\n",
    "    model = TransformedTargetRegressor(regressor=pipe,func=func,inverse_func=invboxcox)\n",
    "    \n",
    "    # Fit model to X and y\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    #Save the model to the designated \n",
    "    with open(f'{data_path}/{model_file}', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "        \n",
    "    print(\"Model Trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function\n",
    "\n",
    "### Make prediction on the held-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='predict_op',\n",
    "    description='prediction pipeline function for Graduate admission',\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "\n",
    "def predict(data_path, predict_file):\n",
    "    \n",
    "    import pickle     # python object for (de)serialization\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # Evaluation metrics\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    # Load the saved BayesianRidge Regressor model\n",
    "    with open(f'{data_path}/{model_file}', 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "    #load the preprocessed data\n",
    "    preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "    train = preprocessed_data['train']\n",
    "    test = preprocessed_data['test']\n",
    "    X = preprocessed_data['X']\n",
    "    y = preprocessed_data['y']\n",
    "        \n",
    "        \n",
    "    # Normally, new/test data does not contain the target variable, \n",
    "    # so we drop it from the test in order to use the trained model on it\n",
    "    x_test = test.drop(['Chance of Admit '], axis=1)\n",
    "        \n",
    "    # make prediction on test data\n",
    "    prediction = model.predict(x_test)\n",
    "        \n",
    "    #Save the prediction to the designated \n",
    "    with open(f'{data_path}/{predict_file}', 'wb') as file:\n",
    "        pickle.dump(prediction, file)\n",
    "        \n",
    "    print(\"Prediction has be saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Classifier\n",
    "\n",
    "### Make prediction on Students Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='predict_classifier_op',\n",
    "    description='prediction pipeline function for Graduate admission',\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "\n",
    "\n",
    "def predict_classifier(data_path, predict_class_file):\n",
    "    \n",
    "    import pickle     # python object for (de)serialization\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # Evaluation metrics\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "        \n",
    "    # Load the saved BayesianRidge Regressor model\n",
    "    with open(f'{data_path}/{predict_file}', 'rb') as file:\n",
    "        prediction = pickle.load(file)   \n",
    "        \n",
    "    ## checking rmse between target variable in main scale\n",
    "#     main_test_rmse = np.sqrt(mean_squared_error(test['Chance of Admit '], prediction))\n",
    "#     print(f'Final RMSE value on test data = {main_test_rmse}')\n",
    "        \n",
    "        \n",
    "    final_pred = pd.DataFrame(prediction,columns = [\"predictions\"], index = None)\n",
    "        \n",
    "    # We create a classifier for the predicted values\n",
    "    # create a threshold >= 0.70\n",
    "    final_pred['result'] = np.where(final_pred['predictions'] >= 0.70, 'You have a high chance of getting admitted', \n",
    "                                        'Your chances of getting admitted is quite low')\n",
    "    return final_pred\n",
    "\n",
    "    \n",
    "    \n",
    "    with open(f'{data_path}/prediction_result.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {}, Actual: {} \".format(prediction, test['Chance of Admit ']))\n",
    "    \n",
    "    print('Prediction Classifier saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a pipeline component from the function\n",
    "\n",
    "#### Convert the function to a pipeline operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preproces lightweight components.\n",
    "preprocess_op = comp.func_to_container_op(preprocess, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create transform lightweight components.\n",
    "transform_op = comp.func_to_container_op(transform, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create training lightweight components.\n",
    "train_op = comp.func_to_container_op(train, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create prrdiction lightweight components.\n",
    "predict_op = comp.func_to_container_op(predict, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create predict_classifier lightweight components.\n",
    "predict_classifier_op = comp.func_to_container_op(predict_classifier, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a client to enable communication with the Pipelines API server.\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain-specific language \n",
    "@dsl.pipeline(\n",
    "    name='University Admission',\n",
    "    description='End-to-end training to predict the likelihood of admission of a new candidate.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def graduate_admission_container_pipeline(\n",
    "    data_path: str,\n",
    "    pipeline_file: str,\n",
    "    model_file: str,\n",
    "    predict_file: str,\n",
    "    predict_class_file: str\n",
    "    \n",
    "):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"volume_creation\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create graduate admission preprocessing component\n",
    "    admission_preprocessing_container = preprocess_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "    # Create graduate admission transform component\n",
    "    admission_transformation_container = transform_op(data_path, pipeline_file) \\\n",
    "                                        .add_pvolumes({data_path: admission_preprocessing_container.pvolume})\n",
    "    \n",
    "    # Create graduate admission training component\n",
    "    admission_training_container = train_op(data_path, model_file) \\\n",
    "                                    .add_pvolumes({data_path: admission_transformation_container.pvolume})\n",
    "    \n",
    "    # Create graduate admission prediction component\n",
    "    admission_prediction_container = predict_op(data_path, predict_file).add_pvolumes({data_path: admission_training_container.pvolume})\n",
    "    \n",
    "    # Create graduate admission prediction classification component\n",
    "    admission_prediction_classifier_container = predict_classifier_op(data_path, predict_class_file) \\\n",
    "                                                .add_pvolumes({data_path: admission_prediction_container.pvolume})\n",
    "    \n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    Graduate_admission_result_container = dsl.ContainerOp(\n",
    "        name=\"Admission prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: admission_prediction_classifier_container.pvolume},\n",
    "        arguments=['head', f'{data_path}/prediction_result.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and run the pipeline\n",
    "\n",
    "- Kubeflow Pipelines lets you group pipeline runs by Experiments. You can create a new experiment, or call `kfp.Client().list_experiments()` to see existing ones. If you don't specify the experiment name, the Default experiment will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "MODEL_PATH = 'graduate_admission_predictor.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = graduate_admission_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=EXPERIMENT_NAME\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"model_file\":MODEL_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,'{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
